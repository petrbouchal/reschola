---
title: "The Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette describes the ideal data analytical workflow used by the Schola Empirica team.

## Objective

#### {.bs-callout .bs-callout-green}

To do analysis and create great reports reproducibly and efficiently.

## Principles

### 1. Reproducible projects

Project structure should be such that it is obvious what happens where and the whole project can be rerun quickly, perhaps on new data.
Ideally, the structure also facilitates efficiency, i.e. during analysis things are not rerun which only need to be done once etc.

### 2. Reproducible reports

Everything we submit to a client/partner/stakeholder should be based on data and code that together reproducibly create the report: no hacks, no manual edits if at all possible, no copy-pasting images from one place to another.

Version control helps us track when each report was created, using what code and what data, and can help us go back and fix things if needed. It also helps us not have mostly duplicate versions of everything lying around.

> Code and data are real.

### 3. Consistent style of outputs

Reports and charts are based on well-designed styles and templates which we use consistently.

### 3. Good documentation

Projects and reports should be self-documenting by their structure, code, workflow, and comments inside.

### 4. Efficiency

Let's not repeat code and keep reinventing the wheel.

## Building blocks

### 1. Project initiation via the project template

The `reschola` package provides an RStudio project template that (a) takes care of setting up your project on Github (if you let it) and (b) creates a default project structure, incorporating key parameters that you give it in setup.

### 2. Default project structure

Feel free to adapt this in any way that works and remains understandable to someone who is not you.

- `shared.R` for shared variables and perhaps functions. By default contains GDrive URL and project title, if set during setup
- `0_retrieve-data.R` helps you download files from your GDrive folder, if set. You can also use it to store code for retrieving other data. This should only hold things which you expect to only run once, or refresh rarely.
- `00_read-data.R` should hold code that reads the data and does any transformations immediately tied to data reading, e.g. setting data types or basic filtering. Again, this is code that you don't expect to change as you work on the actual analysis. You may want to save the result into `rds` files in `data-intermediate` (or `data-input` if it is simply an `rds` mirror of the input data saved for quick access.)
- `000_check-and-process-data.R` (you may wish to number this one `01_*` to signal that you expect to rerun it often with the rest of the analysis, and you may also turn it into an Rmd file if that is more convenient.) This should process 
- `[NN]_*.Rmd` where NN is 01-98 is the actual analysis - may be an exploratory script, a partial analysis, or a report. Expected to be run in the order of their numbering, but ideally key components should work off data saved in `data-input` or `data-intermediate`.
- `data-input` should contain only unaltered input data as downloaded.
- `data-intermediate` should contain processed data files
- `data-output` should contain data that you expect to share externally that are the output of your project
- `charts-output` and `reports-output` for the obvious
- `99_reproducibility.R` by default contains a description of the system and environment used to run the analysis. Use it to store any other information useful for reproducing the analysis (but not passwords etc.)

You should feel free to move code between the analysis and the `00_*`/`000_*` scripts as you discover data transformations that should be made earlier on in the workflow.

Use `build.R` to tie these together - when run, it should rebuild the whole project from scratch, except perhaps downloading data.
You may want to build different versions of `build_*.R` as helpers for running different parts of the workflow while you work.

### 3. Document templates

There are two templates in the package: Schola Word report, and Schola redoc report. 

The Word report is simple: it creates a word document with some nice custom defaults and styles.

The redoc template looks similar - and so does its output - but it has some superpowers. You can [To be finished]

### 4. ggplot2 theme

[To be finished]

### 5. Utilities

## Process

### 1. Start a new project

In RStudio, go to `File > New Project > New directory > Standard Schola Empirica Project`.

Fill in the fields (only directory name is mandatory), switch the Git menu to get a (reschola/your) Github repo of you wish, select other options if needed and click `Create`.

Other ways are possible but this gives you a good starting point and takes care of a lot of the setup hassle for you.

### 2. Download the data

Use `0_retrieve-data.R`; if you have other data retrieval, ideally the code for it should live here.

Do not edit the data by hand.

See [tips](tips.html) for some packages that can help you retrieve data from public sources or other systems.
 
### 3. Read in, check and process the data

Use `00_load-data.R`. Add any other data reading that is needed.

Use `000_check-and-process-data.R`. You might need to move this into an RMarkdown document.

See [tips](tips.html) for packages that can help you set up a structured data checking pipeline.

#### Data and git {.bs-callout .bs-callout-yellow}

You may want to commit some of the summarised/processed data output here once you have done some analysis and are reasonably sure it will not change too often. But generally, data should not be committed, esp. if large or at risk of committing personal information.


### 4. Explore/Analyse the data

I suggest you keep your data exploration in a separate script from your report; often the EDA will happen in the report as you go, but a better process perhaps is to develop bits of your analysis in one script/Rmd and only move bits of code into the report Rmd which is essential for building the report.

An RMarkdown Notebook might be an appropriate format for this.

#### Approaches {.bs-callout .bs-callout-blue}

There is a real trade off here: one way to do it is to work through the analysis in the report script, perhaps hiding most through chunk options (`include =  FALSE`) and only outputting into the final format stuff that is relevant. That way you get a sense of the thought process but also a bloated and circuitous script. Another is to do the analysis in one or more files and only moving bits into the report which are needed there. That way you get a tight report script but at times disconnected from the analytical process.

### 5. Write reports using a template

Use `draft_*` to quickly create a draft using the required template.

See below for how to use the `redoc` template.

In principle, if you want a HTML file you can just switch the format to `html_document` and it should work fine if perhaps the details might differ slightly. See [tips]() on how to get that online.

#### Parameterising

If you expect your report to be rerun in some time with different data or a different parameter, like a changed date or name of something, you can make your report parameterised. See this [brief guide](https://rmarkdown.rstudio.com/lesson-6.html) or a [longer explanation](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html).

Here is an example: [To be finished]

This is also useful if you are running the same report for a number of units of something, e.g. for different waves of research or different geographical units - see [how the Urban Institute does it](https://medium.com/@urban_institute/iterated-fact-sheets-with-r-markdown-d685eb4eafce) 

#### Visualising

Charts should be created using `ggplot2` as far as possible. Use the `schola_theme()` theme.

### 6. Iterate steps 3-6

None of this is a linear process.
The only requirement is that from an external point of view (and that includes you in three months or two years), the process of rebuilding the report(s) and the entire project is linear. 

But as you work, you will find bits of code that belong somewhere else; you will make data transformations in your report that you will then realize you can move to your data transformation script. 
You will load new data in a script and than move that loading code to an earlier script.
That is fine - it will happen gradually through iteration, but the iteration should also move you towards more organised code.

The logic described by Emily Riederer in her [RMarkdown driven development](https://emilyriederer.netlify.com/post/rmarkdown-driven-development/) approach may be helpful here.

In the end, the scripts should follow these principes:

1. each should be able to run separately, in the sense that it doesn't fail, that is
2. it reads its own data (possibly written by a previous script)
3. it should write its data if another script is expected to use them (though ideally this would all be done by a data-transformation script early on)
4. it should load its libraries, shared variable and functions

Don't forget to update the README.md and other documentation as you go, as well as `build.R` and any other `build*.R` scripts you may have. 

Feel free to use git to go back and forth.
Version control is your friend here.
Something broke? You can go back to when it worked.

When a draft report goes out e.g. to stakeholders for feedback, it might be useful to create a [git tag](https://happygitwithr.com/git-basics.html):

> You can also designate certain snapshots as special with a tag, which is a name of your choosing. In a software project, it is typical to tag a release with its version, e.g., “v1.0.3”. For a manuscript or analytical project, you might tag the version submitted to a journal or transmitted to external collaborators. Figure 20.1 shows a tag, “draft-01”, associated with the last commit.

### 7. Iterate the report collaboratively

[To be finished]

### 8. Finalise report

Run `reschola::manage_docx_header_logos()` to replace default Schola logo or add a client/funder logo.

### 9. Prepare project for reuse

[To be finished]

## Tools for implementing good practices

### Tidyverse approach and tidy data

See [R for Data Science](https://r4ds.had.co.nz/) by Hadley Wickham and Garret Grolemund.

The rest mostly draws on [**W**hat **T**hey **F**orgot to teach you about R](https://rstats.wtf), which seems to have the remedy to many common pains of working with R.

### Blank slates

See RStudio part in [setup](setup.html) for the options to set for this.)

### Safe paths

Use the `here` package instead of `setwd()` to [make sure paths just work]().

### File naming conventions

See (Naming things)[https://speakerdeck.com/jennybc/how-to-name-files] by Jenny Bryan.

- machine readable: ASCII, no spaces, sensible separators (`_` between parts, `-` between words)
- human readable: descriptive words in title, consistent logic across files
- plays well with default ordering: 01 x 11, sensible separators, all lowercase, YYYY-MM-DD dates


### Safe storage of secret and confidential information

- use .Renviron for passwords (or look at `keyring`), never hard code them (`usethis::edit_r_environ()`)
- store individual data on team GDrive and only download for analysis; do not commit to git

#### Ignore files in git {.bs-callout .bs-callout-yellow}

You can add files you do not wish to commit to git's '.gitignore' file.
That way, git will not even show those as new/changed. 
This works separately for each repo.

The easiest way to do this is to run e.g. `usethis::use_git_ignore("secret_file.R")`.

You need to commit the `.gitignore` file.

### Style

Currently we have no explicitly agreed style.


## Resources for building reproducible workflows

[Drake](https://github.com/ropensci/drake), [intro slides](https://pkg.garrickadenbuie.com/drake-intro/#1)

[Sharla Gelfand](https://resources.rstudio.com/rstudio-conf-202) on reproducible reporting with RMarkdown at rstudio::conf(2020)
Emily Riederer on [RMarkdown driven development](https://emilyriederer.netlify.com/post/rmarkdown-driven-development/)

Wilson et al. 2016, ["Good Enough Practices in Scientific Computing"](https://arxiv.org/abs/1609.00037)

[`rrtools`](https://github.com/benmarwick/rrtools) for research compendia

[Reproducible Analytical Pipelines](https://ukgovdatascience.github.io/rap-website/)

[Workflowr](https://jdblischak.github.io/workflowr/)
